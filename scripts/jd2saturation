#!/usr/bin/env python
#--coding:utf-8 --
"""
jd2saturation
2017-10-12: Estimation of effect of sequencing depth for loops calling through re-sampling.
"""
__date__ = "2017-10-10"
__modified__ = ""
__email__ = "caoyaqiang@picb.ac.cn"

#general library
import os

#3rd library
import joblib
from joblib import Parallel, delayed
import numpy as np
import pandas as pd

#cLoops
from cLoops.utils import getLogger, jd2saturationHelp
from cLoops.io import parseJd, parseIv
from cLoops.cDBSCAN import cDBSCAN as DBSCAN
from cLoops.ests import estIntSelCutFrag
from cLoops.pipe import checkOverlap, filterClusterByDis, combineTwice
from cLoops.cModel import getIntSig, markIntSig, markIntSigHic

#global settings
global logger


def generateSamplingData(jd, fout, repeats, step, cut):
    """
    Generate re-sampling data in .jd files
    """
    logger.info("Generating re-sampling data.")
    chrom, mat = parseJd(jd, cut)
    n = mat.shape[0]
    fs = []
    ss = np.arange(n)
    for i in np.arange(1.0 / step, 1.0, 1.0 / step):
        m = int(n * i)
        for r in xrange(repeats):
            np.random.shuffle(ss)
            ns = ss[:m]
            nd = mat[ns, :]
            dt = os.path.join(fout, "depth_%s_rep_%s" % (i, r))
            if not os.path.exists(dt):
                os.mkdir(dt)
            fto = os.path.join(dt, "%s-%s.jd" % (chrom[0], chrom[1]))
            joblib.dump(nd, fto)
            fs.append(fto)
    return fs


def singleDBSCAN(f, eps, minPts, cut=0):
    """
    Run DBSCAN to detect interactions for one chromosome.
    #mat is list, every is [ pointId,x,y ]
    """
    dataI, readI, dataS, readS, dis, dss = [], [], [], [], [], []
    key, mat = parseJd(f, cut)
    if len(mat) == 0:
        return key, f, dataI, dataS, list(dis), list(dss)
    #data for interaction records, read for readId
    report = "Clustering %s and %s using eps as %s, minPts as %s,pre-set distance cutoff as > %s" % (
        key[0], key[1], eps, minPts, cut)
    logger.info(report)
    db = DBSCAN(mat, eps, minPts)
    labels = pd.Series(db.labels)
    mat = np.array(mat)
    mat = pd.DataFrame(
        mat[:, 1:].astype("float"), index=mat[:, 0], columns=["X", "Y"])
    nlabels = set(labels.values)
    #collect clusters
    for label in nlabels:
        los = list(labels[labels == label].index)
        sub = mat.loc[los, :]
        #BEDPE format,+1 to escape the error that exact the same start and end
        #2017-05-18, changed to remove such interactions
        if int(np.min(sub["X"])) == int(np.max(sub["X"])) or int(
                np.min(sub["Y"])) == int(np.max(sub["Y"])):
            continue
        r = [
            key[0],
            int(np.min(sub["X"])),
            int(np.max(sub["X"])),
            key[1],
            int(np.min(sub["Y"])),
            int(np.max(sub["Y"])),
            #sub.shape[0],
            #",".join(map(str, los)),
            #los
        ]
        if r[2] < r[4]:
            dataI.append(r)
            readI.extend(los)
        else:
            dataS.append(r)
            readS.extend(los)
    report = "Clustering %s and %s finished. Estimated %s self-ligation reads and %s inter-ligation reads" % (
        key[0], key[1], len(readS), len(readI))
    logger.info(report)
    if len(dataI) > 0:
        dis = mat.loc[readI, "Y"] - mat.loc[readI, "X"]
    if len(dataS) > 0:
        dss = mat.loc[readS, "Y"] - mat.loc[readS, "X"]
    return key, f, dataI, dataS, list(dis), list(dss)


def runDBSCAN(fs, eps, minPts, cut):
    """
    Run DBSCAN to detect interactions for all chromosomes.
    """
    ds = []
    for f in fs:
        d = singleDBSCAN(f, eps, minPts, cut)
        ds.append(d)
    dataI, dataS, dis, dss = {}, [], [], []
    for d in ds:
        if len(d[2]) == 0:
            continue
        dataI[d[0]] = {"f": d[1], "records": d[2]}
        dataS.extend(d[3])
        dis.extend(d[4])
        dss.extend(d[5])
    return dataI, dataS, dis, dss


def runStat(dataI, minPts, cut, fout, hic=0):
    """
    Calling p-values of interactions for all chromosomes.
    """
    ds = []
    for key in dataI.keys():
        d = getIntSig(dataI[key]["f"], dataI[key]["records"], minPts, cut)
        ds.append(d)
    ds = [d for d in ds if d is not None]
    if len(ds) == 0:
        logger.error("Something wrong, no loops found, sorry, bye.")
        return 1
    ds = pd.concat(ds)
    try:
        if hic:
            ds = markIntSigHic(ds)
        else:
            ds = markIntSig(ds)
        ds.to_csv(fout + ".loop", sep="\t", index_label="loopId")
    except:
        logger.warning(
            "Something wrong happend to significance estimation, only output called loops"
        )
        ds.to_csv(fout + "_raw.loop", sep="\t", index_label="loopId")
    return 0


def getLoops(jd, eps, minPts, hic, cut, fout, cd=1):
    logger.info("Getting loops for %s" % jd)
    floop = fout + ".loop"
    if os.path.isfile(floop):
        logger.info("%s exists,return" % floop)
        return
    if cd:  #change minPts according to sampling depth
        #n = os.path.split(os.path.split(jd)[0])[1]
        n = jd.split("/")[-2]
        d = float(n.split("_")[1])
        minPts = int(d * minPts)
    #run cLoops
    dataI, dataS, dis, dss = [], [], [], []
    dataI = {}
    cuts = []
    for ep in eps:
        dataI_2, dataS_2, dis_2, dss_2 = runDBSCAN([jd], ep, minPts, cut)
        if len(dataI_2) == 0 or len(dataS_2) == 0:
            continue
        cut_2, frags = estIntSelCutFrag(np.array(dis_2), np.array(dss_2))
        cuts.append(cut_2)
        dataI_2 = filterClusterByDis(dataI_2, cut_2)
        #combine the first round and second round result
        dataI = combineTwice(dataI, dataI_2)
    cut = min(cuts)
    e = runStat(dataI, minPts, cut, fout, hic)
    return floop


def getSets(f):
    mat = pd.read_table(f, index_col=0)
    s = mat["significant"]
    s = s[s > 0]
    rs = []
    for i in s.index:
        a = parseIv(mat.loc[i, "iva"])
        b = parseIv(mat.loc[i, "ivb"])
        r = [a[0], a[1], a[2], b[0], b[1], b[2]]
        rs.append(r)
    return rs


def getSaturation(fa, fbs, fout):
    ds = {}
    rsa = getSets(fa)
    for f in fbs:
        n = os.path.splitext(os.path.split(f)[-1])[0].split("_")
        d = float(n[1])
        r = int(n[-1])
        rsb = getSets(f)
        c = 0
        for ra in rsa:
            flag = 0
            for rb in rsb:
                if checkOverlap(ra, rb):
                    flag = 1
                    break
            if flag:
                c += 1
        if d not in ds:
            ds[d] = {}
        ds[d][r] = c
    ds = pd.DataFrame(ds) / len(rsa) * 100
    ds.to_csv(
        fout + "_ResamplingRatios.txt", sep="\t", index_label="replicates")


def jd2saturation(jd, fout, eps, minPts, repeats, step, cpu, hic, cut):
    """
    Re-sampling PETs to estimate loops detection saturation.
    jd: .jd file, from cLoops -s 1
    fout: output prefix, str
    eps: eps for cLoops,[int,int]
    minPts: minPts for cLoops,int
    repeats: repeat times for each sampling depth
    step: steps for different sampling, int
    cpu: cpus used to run,int
    hic: whether use hic cutoff for loops calling, boolean 
    cut: distances for filtering PETs for cLoops, int
    """
    if os.path.isdir(fout):
        r = "working directory %s exists, return." % fout
        logger.error(r)
        return
    os.mkdir(fout)
    ft = os.path.join(fout, "%s" % fout)
    floop = getLoops(jd, eps, minPts, hic, cut, ft, cd=0)
    fs = generateSamplingData(jd, fout, repeats, step, cut)
    fs = Parallel(n_jobs=cpu)(delayed(getLoops)(
        f, eps, minPts, hic, cut, os.path.join(os.path.split(f)[:-1])[0])
                              for f in fs)
    getSaturation(floop, fs, fout)


if __name__ == "__main__":
    global logger
    fn = os.path.join(os.getcwd(), "jd2saturation.log")
    logger = getLogger(fn)
    op = jd2saturationHelp()
    if "," in str(op.eps):
        eps = map(int, op.eps.split(","))
    else:
        eps = int(op.eps)
        if eps != 0:
            eps = [eps]
    jd2saturation(op.jd, op.output, eps, op.minPts, op.repeats, op.step,
                  op.cpu, op.hic, op.cut)
    r = "jd2saturation finished. Bye!"
    logger.info(r)
